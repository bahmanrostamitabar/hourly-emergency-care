---
title:  Forecasting short-term hourly Emergency Department arrivals
author:
  - name: Bahmna Rostami Tabar
    email: rostami-tabarb@cardiff.ac.uk
    affiliation: Cardiff Business School
    footnote: 12
  - name: Jethro Browell
    email: jethro.browell@glasgow.ac.uk
    affiliation: University of Glasgow
    footnote: 1
  - name: Ivan Svetunkov
    email: i.svetunkov@lancaster.ac.uk
    affiliation: Lancaster University
    footnote: 1
address:
  - code: Cardiff Business School
    address: Cardiff Business School, Cardiff University, UK
  - code: University of Glasgow
    address: School of Mathematics and Statistics, University of Glasgow, UK
  - code: Lancaster University
    address: Centre for Marketing Analytics and Forecasting, Lancaster University, UK
footnote:
  - code: 12
    text: "Corresponding Author"
  - code: 1
    text: "Equal contribution"
    
abstract: |
   An accurate forecast of Emergency Department (ED) arrivals by an hour of the day is critical to meet patients’ demand. It enables planners to match ED staff to the number of arrivals, redeploy staff, and reconfigure units. This can have many advantages for healthcare staff and the quality of care delivered to patients. In this study, we develop an innovative model based on Generalised Additive Models and an advanced dynamic model based on exponential smoothing to generate an hourly probabilistic forecast of ED arrivals. We compare the forecast accuracy of these models against appropriate benchmarks, including TBATS, Poisson Regression, Prophet, and simple empirical distribution. We use Root Mean Squared Error (RMSE) to examine the point forecast accuracy and assess the forecast distribution accuracy using Quantile Bias, PinBall Score and Pinball Skill Score. Our results indicate that the proposed models outperform their benchmarks for point and probabilistic forecasts. Our developed models can also be generalised to forecast hourly arrivals in other services such as hospitals, ambulances, or clinical desk services.

keywords:  "Emergency Department, Arrivals, Poisson Regression, Probabilistic Forecasting, Generalised Additive Models, Intermittent Exponential Smoothing"  
   
journal: "Annals of Emergency Medicine"
date: "`r Sys.Date()`"
geometry: "top=25mm, left=30mm, right=30mm, bottom=25mm,headsep=10mm, footskip=12mm"
#linenumbers: true
numbersections: true
#output: rticles::elsevier_article
header-includes:  
 \usepackage{adjustbox, float,lscape, bm,amsmath} #use the 'float' package
output:
  bookdown::pdf_book: 
    base_format: rticles::elsevier_article
    citation_package: natbib
bibliography: mybibfile.bib
#biblio-style: authoryear
#csl: elsevier-without-titles.csl
#biblio-style: abbrevnat
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, fig.pos = 'H', message = FALSE, warning = FALSE)
# Make sure you have the latest versions of rmarkdown and rticle
library(tidyverse)
library(lubridate)
library(bookdown)
library(patchwork)
library(tsibble)
library(kableExtra)
library(ggplot2)
library(RColorBrewer)
library(ggridges)
library(hrbrthemes)
library(ggthemes)
library(scales)
library(ggtext)
library(extrafont)
library(feasts)
```


```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../r_script/data-viz.R')
knitr::read_chunk('../r_script/admision_viz.R')
knitr::read_chunk('../r_script/result_viz.R')
```

# Introduction

Forecasting Emergency Department (ED) arrivals is critical for informing staffing and scheduling decisions to meet the needs of patients. Accurate ED demand forecasts contribute to a better decision making process regarding resources allocation and staffing. This is one of the best ways to optimise resources utilisation and minimise related costs. An accurate forecast of patient arrivals is crucial in ED services to depict various courses of action that can result in massive savings in terms of patient lives. Inability to match the staff with the demand might result in patients overcrowding the system, which is a severe problem that causes challenges for the patient flow [@derlet2002overcrowding]. Also, it is related to increasing length of stay [@muhammet2015forecasting], low patient satisfaction, unexpected return visits to services, increased health care costs, inaccuracy in electronic medical records and other [@rostami2020anticipating].

Accurate forecasting of arrivals by the hour of the day enables planners to match staff to meet anticipated patients, reconfigure units and redeploy staff. This has many advantages for both patients, staff and the quality of provided services. Hourly forecasts are required to inform the short-term operational planning for the current and the upcoming shifts of the day. This involves the short-term decision making related to the execution of the delivery process in ED. The combination of an hourly arrival forecast, current staff, being occupied, resource availability and waiting times at ED provide information on the state of the unscheduled care system across the service. Having this complete picture enables the delivery managers to focus on the areas that require intervention to allow the most effective delivery of the service to the patients. However, compared with lower frequency time series forecasting such as monthly, quarterly and yearly, hourly forecasts are challenging because the noise caused by random variation may overshadow any pattern in the time series. Hourly time series generally exhibit multiple seasonal cycles of different lengths: hourly, daily, weekly, and yearly. They may also express nonstationarity, and their profile may change over time. Therefore, an appropriate forecasting model should consider these features to accurately predict hourly demand admissions.

There are few studies that look at forecasting hourly arrivals in ED and other hospital services using historical time series data and/or predictors such as patient characteristics, weather, holidays and public events. These studies use multiple approaches, including Exponential Smoothing [@SvetunkovAdam], Autoregressive Integrated Moving Average (ARIMA) [@hyndman2021forecasting], Autoregressive Conditional Heteroskedasticity (ARCH) [@bollerslev1994arch], Vector Autoregressive model [@lutkepohl2013vector],  TBATS [@de2011forecasting] and Artificial Neural Networks [@hyndman2021forecasting]. However, most of these studies are limited to only predicting future arrivals as a point forecast (a single number), which does not quantify any uncertainty associated with the number of future arrivals. There are few studies that report uncertainty by presenting prediction intervals, but there is no study generating and evaluating the entire forecast distribution of arrivals. Reporting the uncertainty via the forecast distribution is potentially valuable in this setting because the consequences of inadequate staffing are asymmetric. This asymmetry arises because it is preferable to incur a small opportunity cost associated with utilised staff rather than compromise service levels if staff levels are insufficient. Probabilistic forecasts inform decision-makers about exposure to these risks and potentially enable those risks to be managed more efficiently.

Furthermore, if the impact of under- and overstaffing can be quantified, probabilistic forecasts allow `optimal' decisions that balance the cost associated with under- and overstaffing to be calculated. Therefore, in this paper, in addition to generating point forecasts, we also produce and evaluate density forecasts of hourly ED arrivals, comparing several methods for this task. Another drawback of existing studies is that the datasets used are relatively small (e.g. time period of 1-2 years), making it challenging to report the forecast accuracy using robust approaches such as time series cross-validation. Such results might not be generalisable. Additionally, most of the forecasting methods used in these publications do not consider the full extent of the multiple seasonality of hourly ED arrivals. Moreover, all previous publications referenced in this paper are not fully reproducible as underlying data and code are not available.

In this paper, we aim at filling several gaps, and our contributions to the literature are summarised as follows:

1. We produce probabilistic forecasts, in addition to the point estimation, quantifying uncertainties in future hospital admission, and comparing different forecasting methods using a suite of well-established evaluation metrics; 

2. We develop an advanced dynamic model to forecast ED arrivals based on iETS [@Svetunkov2019a] and ETSX models with a modification for multiple frequencies, which produced highly-accurate point forecasts;

3. We develop a novel model to produce a probabilistic forecast of ED arrivals based on Generalised Additive Models for Location Scale and Shape, which accounts for i) the bounded and non-Gaussian distribution of arrivals, ii) multiple seasonalities, weather and holiday effects, and  iii) variation in forecast uncertainty;

4. We benchmark the accuracy of our model against appropriate models used when multiple seasonality is present, i.e. Prophet, TBATS, Poisson Regression, Exponential Smoothing State Space model (ETS) and the simple empirical distribution of the arrivals;

5. We provide data and code enabling reproduction and refinement of the proposed approach and benchmarks. The proposed approach could also be generalised to forecast hourly requirements in other services, such as the number of incidents or call volumes in clinical desk services. 

The rest of the paper is organised as follows: In section \@ref(lit), we provide a brief overview of hourly ED arrival forecasting; In Section \@ref(data), we present the hourly time series of an ED arrival and use various plots to highlight important patterns. In section \@ref(model), we describe the modelling approach and benchmark methods. We then discuss the performance evaluation metrics in section \@ref(accuracy); in section \@ref(result), we present and discuss our results. Finally, we summarise our findings and present ideas for future research in section \@ref(conclusion).



# Research background: hourly ED forecasting {#lit}

Some studies employ models to forecast admissions and arrivals in Emergency Department. The time granularity considered by these studies spans from hourly to yearly. However, given the focus of the paper, we only discuss hourly ED forecasting. 

@hertzum2017forecasting used linear regression, ARIMA, and Naïve to investigate whether accurate hourly accident and emergency department patient arrivals and occupancy forecasts can be generated using calendar variables. @hertzum2017forecasting study found that patient arrival variation is larger across the hours of the day than across the days of the week and the months of the year. In terms of the hour of the day, patient arrivals peaked around noon. For days of the week, Monday is the busiest day, while weekends are the quietest ones. July-August are the months with the highest number of patient arrivals, while January and February have the lowest numbers. They indicate that regression and ARIMA models performed similarly in modelling patient arrivals, while ARIMA outperformed regression models in modelling accident and emergency department occupancy.

@choudhury2020forecasting used ARIMA, Holt-Winters, TBATS, and neural network methods to forecast hourly accident and emergency department arrivals. ARIMA model was selected as the best fit model. Authors claimed that ARIMA provided high and acceptable hourly ED forecasting accuracy, even outperforming TBATS. @Cheng2021 developed an ARIMA model for ED occupancy with a seasonal component and exogenous variables, which outperformed a rolling-average benchmark. They also produce prediction intervals, a form of the probabilistic forecast, which were found to be well-calibrated, a necessary property for such forecasts. 

@morzuch2006forecasting used the Unobserved Components Model (UCM), in which each component of the time series is separately modelled as stochastic. Double-seasonal exponential smoothing and standard Holt-Winters were used to forecast ED arrival for a horizon of 168 hours. The hourly data collected from an ED in Pennsylvania showed no trend and two seasonal cycles: a within-day and a within-week seasonal cycles. The double seasonal model recorded lower RMSEs for all the 168-hour horizons, which was expected due to the strong hourly seasonality of the time series.

@mccarthy2008challenge employed a Poisson log-linear regression model, including independent variables such as temporal factors (e.g., hour-of-day, day-of-week, type-of-day, season, and calendar year), patient characteristics (i.e., age, gender, insurance status, triage level, mode of arrival, and ambulance diversion status) and climatic factors (i.e., temperature and precipitation) to model patient demand for ED services. The authors produced probabilistic predictions in the form of $50\%$ and $90\%$ prediction intervals for the number of hourly arrivals. Hourly data of ED arrivals in the 1-year study period was modelled and analysed, and it was suggested that the model could be used for forecasting. However, model evaluation was performed in-sample on only one year of data, so it is unclear how this approach would perform in a forecasting setting or compare to simpler approaches. However, the length of the time series in this study was very short (only one year), which did not allow for a rigorous out-of-sample evaluation.

@schweigler2009forecasting investigated whether time series methods could accurately generate short-term forecasts of ED bed occupancy. A year-long dataset of hourly ED bed occupancy was collected from three facilities. The authors implemented an hourly historical average model, SARIMA model, and sinusoidal model with autocorrelated error for each facility. The historical average model was based on the mean occupancy for each site, for each hour of the day, while the sinusoidal model was based on four parameters: an AR term, a sine coefficient, a cosine coefficient and an intercept. They evaluated the forecast accuracy of four and twelve hours forecast horizons using RMSE. They found that both SARIMA and the sinusoidal models outperformed the historical average (for example, at site 2, the two models improved by $33\%$ the 12-hour forecasts generated by the historical average).

@kim2014predicting compared different univariate and multivariate time series forecasting techniques to predict patient volume for a Hospital Medicine programme. The study evaluated linear regression, exponential smoothing, ARIMA, SARIMA, Generalized Autoregressive Conditional Heteroskedasticity (GARCH) and  Vector Autoregressive (VAR) models to forecast for 4 and 24 hours ahead. They used Mean Absolute Percentage Error (MAPE) to report the forecast accuracy. The authors found that the ARIMA outperformed all the other models.

Table \@ref(tab:summarylit) summarize the relevant papers.

```{r summarylit, cache=FALSE}
li <- readxl::read_xlsx("../table/table.xlsx")
knitr::kable(li, booktabs = T, linesep = "",caption = "Summary of studies in forecasting hourly arrivals in Emergency Department") %>% kableExtra::kable_styling(latex_options = c("scale_down","hold_position"), font_size =11) %>% column_spec(3,"6em") %>%
  column_spec(4,"4em") %>% column_spec(6,"10em")  %>% column_spec(7,"8em") %>%  kableExtra::landscape()
```


@asheim2019real developed a Poisson time-series regression model with continuous day-of-week and week-of-year effects to implement a real-time system that could forecast ED arrivals on 1, 2, 3 hours ahead. Measuring the accuracy using the MAPE, @asheim2019real noticed that significant improvement happened when the time of notification was incorporated into the model, especially in the one-hour horizon.

@cheng2021forecasting used one year of ED visits time series to evaluate the Rolling Average, SARIMAX, ARIMA, VAR and Holt-Winter to forecast ED occupancy up to 4-hours ahead. The forecast accuracy is evaluated using Mean Squared Error (MSE), Mean Absolute Error (MAE) and MAPE for point forecast and coverage for prediction intervals of $80\%$ and $95\%$. They show that SARIMAX provides a more accurate forecast of hourly ED occupancy.

According to the studies mentioned above, it can be said that they have shown complications in forecasting hourly patient accident and emergency department visits, and the application of forecasting hourly patient visits is not well established. Some of the studies claimed that the accuracy of forecasting models on hourly accident and emergency department data is low [@boyle2012predicting; @hertzum2017forecasting], while others mentioned that the accuracy of ED hourly forecast is at the acceptable level [@choudhury2020forecasting; @mccarthy2008challenge;  @schweigler2009forecasting].

There are a few limitations in the literature which encourage us to undertake this research and examine different forecasting approaches:

(i) Current approaches to forecast hourly ED arrivals do not fully consider the feature of data such as multiple seasonal cycles and changing profile over time;
(ii) Almost all research studies produce point forecasts and, at best, report prediction intervals. There is a lack of studies presenting the entire forecast distribution of hourly ED arrivals that better represent the uncertainty of future arrivals, providing a holistic picture of future demand for a planner;
(iii) most studies are not reproducible, as it is almost impossible to reapply the approaches without the help of the authors of those papers;
(iv) studies are limited in terms of the length of historical data used for training purposes and forecast performance evaluation and
(v) some studies in this area lack a rigorous experimental design, i.e. they do not use benchmark methods or report forecast accuracy.


# Preliminary analysis {#data}

Data used in this study comprises counts of patients’ arrival times at one of the largest ED units in the UK between April 2014 and February 2019, extracted from the ED administrative database of the hospital. We aggregated the patients’ arrival times to obtain hourly arrivals, which are used in this study. Figure \@ref(fig:hourly-plot-ridge) illustrates the distribution of arrivals for each hour of the day and the day of the week. Although the data is noisy, it reveals some systematic patterns.

```{r hourly-plot-ridge, fig.cap= "Distrubution of admission per hour and day of the week", fig.align='center',fig.width= 6.5,fig.height=5.5}
```

It is clear that the number of arrivals has a sub-daily structure, similar to the one summarised by @hertzum2017forecasting. The ED arrivals decrease between mid-night and early morning and then increase until the evening, decreasing after that again. It is also clear that ED service gets systematically more visits on Mondays between 8 a.m. and 5 p.m. Moreover, the number of arrivals around mid-night is slightly higher for Saturday and Sunday.

Figure \@ref(fig:hourly-plot-ridge) also highlights that there is significant skewness for almost every hour of the day that varies with time-of-day, and which should be accounted for in forecasting methods. Some skewness might be related to holidays and special events. It is also clear that arrivals are less volatile between mid-night and early morning.

```{r seasonplot-dofw, fig.cap= "Seasonal plot: day of week arrivals", fig.align='center',out.width="70%"}
```

Figure \@ref(fig:seasonplot-weekofyear) illustrates the daily seasonal plot, with x-axis representing the day-of-week and y-axis the ED arrivals.  There is a line for each week (i.e. 7 days) from Monday to Saturday, and lines are color-coded by week of year to show the weekly cycle. It is clear that there is a large jump in patient arrivals on Monday followed by Saturday each week. This shows that there are significantly more arrivals on Mondays and Saturdays comparign to the rest of the week. This might be due to the closure of General Practitioners outpatient clinics over the weekend. 

```{r seasonplot-weekofyear, fig.cap= "Week of year arrivals", fig.align='center',out.width="70%"}
```

Figure \@ref(fig:seasonplot-weekofyear) highlights the week of year seasonality in the ED arrivals. We observe that arrivals are significantly lower from week 29 to 35, which corresponds to the summer period. Moreover, the number of arrivals are lower at beginning and the end of the year. The highest arrivals are in week 39-42.

```{r date-plot, fig.cap= "Daily arrivals", fig.align='center',out.width="70%"}
```

Figure \@ref(fig:date-plot) presents the time plot of daily arrivals. Each point represents one day, and points are shape-coded by day-of-week to show the weekly cycle. The figure shows more arrivals on Mondays compared to other weekdays. Moreover, we can observe a long-term trend line as well as the significant effect of some holidays. We can see that arrivals near Christmas and New Year's day are significantly lower than other days of the year. Moreover, Figure \@ref(fig:date-plot) allows us to identify the impact of special event on arrivals. For instance, we see that the number of arrivals are significantly low for 01-03 of March 2018. These days correspond to the Storm Emma with heavy snowfall that resulted in travel disruption, mass power outages and schools closed in the UK [@stormemma2018].

Based on ED time series analysis illustrated above and literature review, we should consider models that can take the following into account:

- Daily, weekly, and annual seasonalities,

- Long-term trend,

- Calendar events, such as holidays,

- Lags of calendar events to accommodate the potential changes in demand the next day after a holiday,

- Other events, such as sporting fixtures,

- Weather effects, such as temperature and precipitation.

We propose several forecasting models that account for the structures outlined above.

# Model building {#model}

## Naive/Climatology {#climatology}

We start with one of the simplest forecasting approaches used in practice - the one that assumes that in the next few hours, everything will be the same as in the similar hours of a similar day in the past. This is called "Naïve". In our case, given that we need a distribution of values, we will use a modified approach, used in climatology, where the empirical distribution of the hourly arrival time series is used to forecast the future arrival distribution [@la2021new]. 

<!-- Empirical distribution for same hour-of-day and day-of-week as target time. -->


## Poisson Regression

Regression is one of the most popular forecasting methods that uses explanatory variables to predict a variable of interest (in our case, the ED arrivals). The classical linear regression model is formulated as

\begin{equation}
  {y}_t = \mathbf{x}_t' \boldsymbol{\beta} + \epsilon_t ,
(\#eq:linearRegression)
\end{equation}

where $\mathbf{x}_t$ is the vector of explanatory variables, $\boldsymbol{\beta}$ is the vector of parameters, $\epsilon_t$ is the error term, which is typically assumed to follow Normal distribution with zero mean and a fixed variance, and $t$ is the time index. However, in the context of healthcare and ED arrivals, the assumption of Normality is unrealistic, because the number of admitted patients is integer and non-negative. So the linear regression model should be substituted by some other model. One of the models that is frequently used in practice is the Poisson regression [see for example, @mccarthy2008challenge], which can be summarised as

\begin{equation}
  \begin{aligned}
    & {y}_t \sim \mathrm{Poisson} \left( \lambda_t \right) \\
    & \log \lambda_t = \mathbf{x}_t' \boldsymbol{\beta}
  \end{aligned} \quad .
(\#eq:PoissonRegression)
\end{equation}

The logarithm in \@ref(eq:PoissonRegression) is needed in order to make sure that the parameter of Poisson distribution is always positive. This model can be estimated via maximisation of the likelihood function based on Poisson mass function <!--[reference needed]--> . When it comes to selecting explanatory variables for the model, there is no single correct answer, and the decision needs to be done for each specific case. In our experiment, we will only include dummy variables, capturing a variety of calendar events:

1. Hour of day,
2. Day of week,
3. Week of year,
4. Holidays (such as Christmas, New Year etc),
5. 24 hours lags of holidays.

The variables (1)--(3) allow modelling the seasonal patterns on the appropriate level of detail throughout the year, while (4) covers the changes in admittance due to calendar events. Finally (5) is needed in order to capture the potential phenomenon of change in admittance after the holiday (e.g. people might try not to go to hospital on Christmas eve and thus will go the next day). This model assumes that all these effects are deterministic and do not change over time, but the exponentiation in \@ref(eq:PoissonRegression) introduces an interaction effect between dummy variables, so that the 3pm on Monday in January will be different from 3pm on Monday in July, although the parameters for hour of day and day of week are fixed and do not change over time. We use `alm()` function from `greybox` package [@Svetunkov2021Greybox] for R [@RTeam2021] for the experiments and denote this model as "Poisson Regression".


## ETS - Exponential Smoothing

@Hyndman2008b developed a state space approach for exponential smoothing models, according to which the model can have a set of components, including different types of Error, Trend and Seasonal component (thus *ETS*). Given the popularity of ETS model in forecasting community, we decided to include the basic ETS(A,N,A) model with the seasonal component with frequency 24 (hour of day) as a benchmark. This was done using `adam()` function from `smooth` package [@Svetunkov2021Smooth] for R and denote as *ETS*. This model does not capture the day of week or week of year effects, does not include explanatory variables, but its seasonal component and level change over time. This model is included as a benchmark, only to see how the other models perform in comparison with it.

## Prophet

Prophet is a forecasting procedure created by Facebook [@taylor2018forecasting] that accounts for multiple seasonality, piecewise trend and holiday effects. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well. Prophet works well on daily data seen in Facebook. It is robust and automated, making it easy to learn for beginners. The implementation may be less flexible than other methods. The model itself relies on Multiple Source of Error state space model, originally proposed by @Kalman1960. The model is incorporated using corresponding implementation of the Fable package in R. We use the `prophet()` function from the `fable` package [@fable2020]. Note that the input data is assigned with an hourly and daily seasonality.


## TBATS

@de2011forecasting proposed a model to deal with time series exhibiting multiple complex seasonalities called "TBATS". It includes a Box-Cox Transformation, ARMA model for residuals and a trigonometric expression of seasonal terms. The latter not only gives the model more flexibility to deal with fractional seasonality but also reduces the parameters of model when the frequencies of seasonalities are high. We fit a TBATS model using the `tbats()` function from the `forecast` package in R [@forecastpackage2020].


## ADAM: multiple seasonal iETSX

@SvetunkovAdam2021 proposed a framework for dynamic models called the Augmented Dynamic Adaptive Model (ADAM). This framework encompasses ARIMA [@Box1976], ETS [@Hyndman2008b] and regression, supporting multiple frequencies, non-normal distributions and intermittent demand [@Svetunkov2019a]. Based on this framework, we use Gamma distribution for ETS(M,N,M) model with frequencies 24 (hour of day) and 168 (hour of week), adding dummy variables for week of year, holidays and lagged holidays. This way we update the hour of day and day of week seasonal indices, keeping the week of year one fixed, thus reducing the number of estimated parameters. Given that the data exhibits randomly occurring zeroes, we use the direct probability model developed by @Svetunkov2019a to treat those values. This model can be formulated as a set of the following equations:

\begin{equation}
	\begin{aligned}
	  & y_t = o_t z_t \\
		& \log z_t = \log l_{t-1} + \log s_{1,t-24} + \log s_{2,t-168} + \mathbf{x}_t' \boldsymbol{\beta} + \log \left(1 + \epsilon_{t} \right) \\
		& \log l_{t} = \log l_{t-1} + \log( 1  + \alpha \epsilon_{t}) \\ 
		& \log s_{1,t} = \log s_{1,t-m} + \log( 1  + \gamma_1 \epsilon_{t}) \\
		& \log s_{2,t} = \log s_{2,t-m} + \log( 1  + \gamma_2 \epsilon_{t}) \\
		& o_t \sim \text{Bernoulli} \left(\mu_{a,t} \right) \\
		& a_t = l_{a,t-1} \left(1 + \epsilon_{a,t} \right) \\
		& l_{a,t} = l_{a,t-1}( 1  + \alpha_{a} \epsilon_{a,t}) \\
		& \mu_{a,t} = \min(l_{a,t-1}, 1)
	\end{aligned} ,
	(\#eq:ADAMModel)
\end{equation}

where $\alpha$, $\beta$, $\gamma_1$, $\gamma_2$ and $\alpha_a$ are the smoothing parameters, defining how adaptive the components of the model should be, $l_t$ is the level component for the demand sizes, $s_{1,t}$ and $s_{2,t}$ are the seasonal components, $\boldsymbol{\beta}$ is the vector of parameters for the explanatory variables, $o_t$ is the binary variable, which is equal to one, when demand occurs and to zero otherwise, $l_{a,t-1}$ is the level component for the occurrence part of the model, and $\left(1+\epsilon_t \right) \sim \Gamma(s^{-1}, s)$, where $s=\frac{1}{T} \sum_{t=1}^{T} e_{t}^2$ is the scale of the distribution. Finally, $a_t$ is an unobservable series, underlying the occurrence part of the model and $(1 + \epsilon_{a,t})$ is an unobservable error term for $a_t$. @Svetunkov2019a discuss how to estimate such a model. We expect this model to perform on par with the Poisson regression, potentially outperforming it in some instances, due to the dynamic parts of the model (level and seasonal components). Although the data is integer-valued, we expect that Gamma distribution will be a good approximation for it. If integer-valued quantiles are needed, then rounding up can be done for them (see \@ref(quantilesceiling) for the explanation). This model is implemented in `adam()` function from `smooth` package [@Svetunkov2021Smooth] for R and is denoted in our experiment as "ADAM-iETSX".


## GAMLSS

If we assume that our predictive distribution follows a given parametric distribution, as in Poisson regression discussed above, the forecasting task becomes ones of predicting the future values of that distribution's parameters. Generalised Additive Models for Location, Scale and Shape (GAMLSS) are distributional regression models where the parameters are modelled as additive functions of explanatory variables. This provides a powerful and flexible framework for probabilistic forecasting, provided that a suitable distribution and additive model structures can be found. In practice, this means employing expert judgement and experimenting with various distributions and evaluating their suitability using available training data.

Let $F_t(y_t)$ be a predictive cumulative probability distribution of $y_t$. In a distributional regression context, $F_t(y_t)$ is modelled via a parametric model, $F(y_t|\bm \theta_t)$, where $\bm \theta_t$ is an $m$-dimensional vector of parameters. In a GAMLSS framework of @Rigby2005 the elements $j=1,...,m$ of $\bm \theta_t$ are modelled as
<!-- If we assume that our predictive distribution follows a given parametric distribution, the forecasting task becomes ones of predicting the future values of that distribution's parameters. Generalised additive models for location, scale and shape (*GAMLSS*) are distributional regression models where the parameters of the assumed distribution may be modelled as additive functions of explanatory variables. This provides a powerful and flexible framework for probabilistic forecasting, provided that a suitable distribution and additive structures can be found. In practice, this means experimenting with various distributions and evaluating their suitability using available training data. -->
<!-- Let $y_t$ be the number of attendances in time period $t$ and indicate with $F_t(y_t)$ its predictive cumulative probability distribution. In a distributional regression context, $F_t(y_t)$ is modelled via a parametric model, $F(y_t|\bm \theta_t)$, where $\bm \theta_t$ is an $m$-dimensional vector of parameters. In a GAMLSS framework [@Rigby2005] the elements $j=1,...,m$ of $\bm \theta_t$ are modelled via -->

\begin{equation}
    g_j(\theta_{j,t})=\mathbf{A}_{j,t} \bm{\beta}_j + \sum_{i} f_{j,i}({\bm x}^{S_{j,i}}_t), \;\;\; \text{for} \;\;\; j = 1, \dots, m,
	(\#eq:basicGAM)
\end{equation}

where $g_j$ is a monotonic link function, $\mathbf{A}_{j,t}$ is the $t$-th row of the design matrix $\mathbf{A}_j$, $\bm \beta_j$ is a vector of regression coefficients, $\bm x_t$ is a $d$-dimensional vector of covariates and $S_{j,i} \subset \{1, \dots, d\}$ **is ...**. If $S_{j,i} = \{1, 3\}$, then following our notation ${\bm x}_{t}^{S_{j,i}}$ is a two dimensional vector formed by the first and third elements of $\bm x_t$. Each $f_{j,i}$ is a smooth function, constructed as

\begin{equation}
    f_{j,i}(\bm x^{S_{j,i}}) = \sum_{k=1}^{K_{j,i}} b^{ji}_k (\bm x^{S_{j,i}}) \beta_k^{ji},
    (\#eq:smmothfunction)
\end{equation}

where $b^{ji}_k$ are spline basis functions of dimension $\vert S_{j,i} \vert$, while $\beta_k^{ji}$ are regression coefficients. The smoothness of each $f_{j,i}$ is controlled via ridge penalties, the definition of smoothness being dependent on the type of effect and penalty being used. See @Wood2017 for a detailed introduction to *GAM/GAMLSS* models, smoothing splines bases and penalties.

As our data are counts, the natural starting point is the Poisson distribution with an additive model for $\log \lambda_t$ of the form
<!-- !!!! IS: I'm not sure that we need CDFs and PMFs, because they are well known. !!!! -->

<!-- \begin{equation} -->
<!--   F_t(y_t,\lambda_t) =  \frac{\Gamma(\lfloor y_t + 1  \rfloor,\lambda_t)}{\lfloor y_t \rfloor} -->
<!--   (\#eq:poissonreg) -->
<!-- \end{equation} -->

<!-- where we consider an additive model for $\lambda_t$ of the form -->

\begin{equation}
  \log(\lambda_t) = \sum_{i=1}^7 \beta_i \delta(D_i(t)-i) + \sum_{j=1}^7 D_j(t) f_j(H(t)) + t f_\text{Y}(Y(t)) + f_\text{Temp}(Y(t),C_t) \quad .
 (\#eq:additivemodel)
\end{equation}

The functions $H(t)$, $D(t)$ and $Y(t)$ return the hour of the day (1--24), day of the week (1--7), and day of the year (1-366) at time $t$, respectively, and $C_t$ is the temperature at time $t$.


However, experiments on the training data reveal that calibration of forecasts based on the Poisson distribution is poor, suggesting that the shape of the distribution is unsuitable for the present application. In particular, we observe that forecast uncertainty appears to vary depending on the time of day and possibly other explanatory variables. Therefore, we consider more flexible, two parameter distributions so that we are able to specify additive models for both location and scale parameters, specifically the truncated Normal distribution, with truncation at 0. The resulting density forecasts are given by

\begin{equation}
F_t(y_t,\mu_t,\sigma_t) =  \frac{\Phi\left( \frac{y_t-\mu_t}{\sigma_t} \right) - \Phi\left( \frac{-y_t}{\sigma_t} \right)}{1 - \Phi\left( \frac{-y_t}{\sigma_t} \right)}
(\#eq:truncatedn)
\end{equation}

with additive models

<!-- \begin{align*} -->
<!-- \mu_t = &  \sum_{i=1}^10 \beta_i D^{+}_i(t) + \sum_{j=1}^10 D^{+}_j(t) f_j(H(t)) + \sum_{k=1}^2 \beta_{10+k} S_k(t) + \sum_{l=1}^2 S_l(t) f_l(H(t)) + \\ -->
<!-- t + f_\text{Y}(Y(t)) + f_\text{Temp}(Y(t),C_t) \quad, \\ -->
<!-- log(\sigma_t) & = \sum_{i=1}^10 D^{+}_i(t) f(H(t)) -->
<!-- (\#eq:additivemodel_NOtr_v2) -->
<!-- \end{align*} -->

\begin{align*}
	& \mu_t = \sum_{i=1}^10 \beta_i D^{+}_i(t) + \sum_{j=1}^10 D^{+}_j(t) f_j(H(t)) + t + f_\text{Y}(Y(t)) + f_\text{Temp}(Y(t),C_t) \\
	& log(\sigma_t) = \sum_{i=1}^10 D^{+}_i(t) f(H(t))
\end{align*}
	
for the mean and variance parameters. We also performed experiments with the truncated $t$ distribution and negative binomial distribution, but these did not results in forecasts as well calibrated as the truncated normal.




<!-- Truncated Normal... -->


<!-- <!-- \begin{equation} --> 
<!-- <!--   F_t(y_t,\mu_t,\sigma_t) =  \frac{\Phi\left( \frac{y_t-\mu_t}{\sigma_t} \right) - \Phi\left( \frac{-y_t}{\sigma_t} \right)}{1 - \Phi\left( \frac{-y_t}{\sigma_t} \right)} --> 
<!-- <!--  (\#eq:truncatedn) --> 
<!-- <!-- \end{equation} --> 

<!-- with -->

<!-- \begin{align*} -->
<!--   \mu_t = &  \sum_{i=1}^7 \beta_i \delta(D_i(t)-i) + \sum_{j=1}^7 D_j(t) f_j(H(t)) + t f_\text{Y}(Y(t)) + f_\text{Temp}(Y(t),C_t) \quad, \\ -->
<!--   log(\sigma_t) & = f(H(t)) . -->
<!-- \end{align*} -->

<!-- Truncated $t$ distribution... -->


<!-- Negative binomial... -->


# Forecast performance evaluation {#accuracy}

In order to assess performance of models, we evaluate predictive quantiles at probability levels 0.05 to 0.95 in steps if 0.05, and conditional expectations for 0 to 48 hours ahead produced by each model. We forecast upto 48 hours because this is the operational horizon in the ED, for which it is possible to make short-term changes in the shifts for nurses and doctors. The forecasts are produced every 12 hours for the holdout of 365 days in a rolling origin fashion [@Tashman2000], resulting in 727 origins. Based on these values, several error measures are calculated to evaluate the performance of models in terms of specific quantiles and in terms of expectation. The latter is measured via Root Mean Squared Error (RMSE)

\begin{equation}
  \mathrm{RMSE} = \sqrt{\frac{1}{h} \sum_{j=1}^h e_{t+j}^2} ,
  (\#eq:RMSE)
\end{equation}

where $h$ is the forecast horizon and $e_{t+j}$ is the point forecast error $j$ steps ahead.

The objective of density forecasts is to be as sharp as possible while remaining reliable/calibrated [@gneiting2007probabilistic]. A forecast is said to be sharp if the predictive distribution has a relatively small spread, indicating low uncertainty, which is valuable to decision makers provided the forecast is calibrated. Calibration, also called reliability, is the property that forecast probabilities match the observed frequency of realisations. If a forecast is calibrated, then, for example, $20\%$ of observations should fall below the $\alpha=0.2$ predictive quantile (with some tolerance based on the finite sample size). This property is necessary for forecast probabilities to be used in quantitative decision-making. Calibration is typically evaluated visually using reliability diagrams, which plot the nominal coverage, $\alpha$, against observed frequency mean ($\mathbf{1}(y_{t}\leq q_{\alpha,t})$
<!-- IS: Check the \mathbb thingy - it didn't work out in pdf // JB: I changed to mathbf, lets see how it looks... -->
). We use several scores to assess the quantile performance of models.

First, in order to measure quantile performance, we need to calculate the pinball score, which is a strictly proper score used to evaluate quantile forecasts and is the discrete form of the Continuous Rank Probability Score [@hyndman2021forecasting]. It rewards sharpness and penalises mis-calibration, so measures all-round performance, however, calibration should still be verified separately. Furthermore, The Pinball Score for an individual quantile matches the loss function minimised in quantile regression model. The Pinball Score is given by

\begin{equation}
    \text{Pinball} = 
    \frac{1}{T|\mathcal{A}|} \sum_{\alpha \in \mathcal{A}} \sum_{t=1}^T
 \left(q_{\alpha,t} - y_{t} \right)
 \left(\mathbf{1}(y_{t}\leq q_{\alpha,t})-\alpha \right) ,
 (\#eq:pinball)
\end{equation}
where $\mathcal{A} = \{0.05,0.1,...,0.95\}$ is the set of quantiles being estimated.

To compare model performance, and the significance of any apparent difference in performance, we will use skill scores, which can be calculated for any metric via:

\begin{equation}
  \mathrm{Skill} = \frac{M_\mathrm{ref} - M}{M_\mathrm{ref}} (\#eq:skillscore)
\end{equation}

where $M$ is the metric's value for the method being considered, $M_\mathrm{ref}$ is the metric's value for a reference method. The skill score show us by how many percent the reference approach is worse than the one under consideration.
<!-- and $M_\mathrm{perf}$ is the metrics value for the `perfect' method, which is zero in the case of RMSE and Pinball. -->
We will use bootstrap re-sampling of skill scores to determine if apparent differences in forecast performance (i.e. positive or negative skill) are significantly different from zero [@Efron1981Bootstrap]. Here we use the best performing simple benchmark, Climatology (explained in Subsection \@ref(climatology)), as the reference model, and employ a block-bootstrap with blocks of length 24h in order to account for temporal correlations of the underlying data [@hongyi1996bootstrapping;@Bergmeir2016303].

Finally, we have calculated the computational time for one iteration on the first rolling origin to compare the speed of each function. All functions were re-estimated on each iteration. ADAM and Poisson regression estimated the parameters taking as the pre-initials the ones obtained in the initial model application to the data in the first origin. This allowed to speed up the computation for these two models. The initial estimation of ADAM took approximately one hour and 25 minutes. Each step in the experiment took the time shown below.


# Results {#result}

The data is portioned into training (from 2014-04-01 to 2018-02-28) and test (from 2018-03-01 to 2019-02-28) sets, with all model development and hyper-parameter tuning performed using training data only. The rolling origin advances in 12 hour steps, and the forecast horizon is set for 48 steps ahead.


Figure \@ref(fig:Pinball) presents pinball score aggregated across forecasting horizons for each quantile. It shows that the difference in performance among the models mainly comes from the middle of the distribution, and somewhat form the upper tail. There is very little difference in performance for the lower tail. This is kind of interesting, and reassuring that the better models are batter at probabilities that matter more to decision makers.

Probabilistic forecasts are evaluated following the principle of *sharpness subject to calibration*, meaning that the sharper forecast is prefered provided that it is calibrated. Mis-calibrated forecasts are unsuitable for use in decision-making so should be excluded. Calibration is evaluated visually in Figure \@ref(fig:quantile-bias), which highlights a systematic negative bias accross all probability levels in many models, with only the truncated normal and $t$ family GAMLSS models (NOtr-1, NOtr-2, Ttr-2) and ADAM-iETSX models showing good calibration across most probability levels. Notably, both benchmarks exhibit negative quantile bias as they struggle to capture the long term trend of increasing attendance. For a user, this could result in poor staffing decisions as historic data fail to accurately charecterise present conditions.


```{r Pinball, fig.cap= "Pinball loss values over different quantiles.", fig.align='center',out.width= "90%"}
```


<!-- ```{r lead-time-rel, fig.cap= "Pinball values over different forecast horizons.", fig.align='center',out.width= "90%"} -->
<!-- ``` -->


<!-- ```{r Reliability, fig.cap= "Reliability score, showing the relation between empirical and theoretical quantiles.", fig.align='center',out.width= "90%"} -->
<!-- ``` -->

```{r quantile-bias, fig.cap= "Quantile bias vs the nominal quantiles.", fig.align='center',out.width= "90%"}
```

<!-- IS: round values to 3 or 4 decimals -->

```{r tab-results, out.width= "50%"}
results_table <- read_rds("results_table.rds")
knitr::kable(results_table, booktabs = T, 
             linesep = "",caption = "Summary of studies in hourly emergency care forecasting") %>% kableExtra::kable_styling(latex_options = c("hold_position"), font_size =11)
```

<!-- Why NAs for RMSE?? -->

Evaluation metrics from the test period are presented in Table \@ref(tab:tab-results). They are ordered by Quantile Bias. The five models identified above have a Quantile Bias of 0.014 or less, which is substantially lower than the next group of forecast with Quantile Biases of 0.037 and above, ETS being the only exception with a value of 0.019.


```{r Skill-rel2bench-reduced, fig.cap= "Skill score ...", fig.align='center',out.width= "70%"}
```

 Figure \@ref(fig:Skill-rel2bench-reduced) illustrates the trade-off between calibration and pinball skill. <!-- We may need to expand this-->


<!-- ```{r rmse, fig.cap= "RMSE ...", fig.align='center',out.width= "70%"} -->
<!-- ``` -->

```{r lead-time-rmse, fig.cap= "RMSE ...", fig.align='center',out.width= "70%"}
```

Figure \@ref(fig:lead-time-rmse) reports the RMSE for each forecast horizon. It illustrates the times of day that are harder to predict – morning pick-up and afternoon peak. <!-- We may need to expand this-->

<!-- ```{r time, fig.cap= "Running time ...", fig.align='center',out.width= "70%"} -->
<!-- ``` -->

One more thing to notice is that the ADAM-iETSX model with rounded up quantiles did not perform better than the simpler one with continuous ones. This implies that the rounding is not necessary in general, but if integer values are needed (for example, to decide how many nurses to have), then using the continuous model and then rounding up the quantiles could be considered as a reasonable strategy.


```{r time-accuracy, fig.cap= "Running time vs. forecast accuracy", fig.align='center',out.width= "70%"}
```

Figure \@ref(fig:time-accuracy) reports the forecast accuracy of each approach versus the computational time required to generate the forecast for a given forecasting horizon of 48 hours. 
<!-- We need to explain this once we have th result for RMSE -->
<!-- ``` -->

# Conclusion {#conclusion}

Short-term forecasting of arrivals at emergency departments is an important element of staff and resource management in hospitals. Furthermore, due to the asymmetric impact of having an excess of shortage of resources, especially in emergency departments, quantifying forecast uncertainty is also of value as it enables planners to manage associated risks. Here, we have developed methods for producing probabilistic forecast of hourly arrivals up to 48 hours ahead, comparing different state-of-the-art approaches.

Two approaches produced highly accurate, calibrated probabilistic forecasts, one time series method and one based on distributional regression. The first is ADAM-iESTX, which is an extension of exponential smoothing incorporating seasonality and assuming a Gamma predictive distribution. The second, labeled NOtr-2, regressed the two parameters of a truncated (at zero) normal distribution on features date and time features, and temperature. Both approaches produced calibrated probabilistic forecasts, but the point prediction produced by ADAM-iESTX produced forecasts had a lower RMSE than NOtr-2, and NOtr-2 produced forecasts with a lower pinball score. This suggests that the latter may be preferred if the whole distribution is used in decision-making.

Having compared the performance of a wide range of methods, we make the following observations: the choice of distribution assumed for probabilistic forecasts, and choice of model features, are as if not more important than the type of model employed; methods based on quantile regression, which do not assume a parametric distribution for forecasts, do not perform as well as those based on parametric distributions; and the best performing models handled the non-negative and skewed nature of the data automatically without need for post-processing. These observations reflect the characteristics of the data, which is representative of ED arrivals, but determining the extent to which they generalise is beyond the scope of this article. Furthermore, methods based on continuous valued distributions are not adversely affected by the fact that the data are integer-valued. Rounding up predictive quantiles to the next integer does not make predictions worse.

Finally, we have found that out-of-the-box models, those which require minimal tuning or manual development, do not perform as well as well-considered statistical methods. The popular TBATS, Prophet and Gradient Boosting Machine algorithms perform poorly compared to ADAM-iETSX and NOtr-2, and even the benchmarks. Of the models requiring a modest amount of user input and expertise, exponential smoothing (ETS) was found to perform well. ETS produces reasonably well calibrated forecasts, in contrast to the benchmarks which were poorly calibrated, and highly accurate point forecasts. However, its probabilistic forecasts were considerably worse than  NOtr-2 in terms of pinball score.

Probabilistic forecasting opens the door to more sophisticated resource management in healthcare settings by providing decision-makers with uncertainty information and enabling quantitative risk management. Linking forecasts of arrivals with upstream (ambulance call-outs) and downstream (length of stay, medical outcomes) analytics offers an opportunity to improve forecast skill, and may also be necessary to maximise benefits though more holistic decision-making.


Further research is required to investigate the practical benefits of the probabilistic forecasts in the healthcare and how they should be used to inform planning and decision making. This may require employing discrete simulation or new-vendor problem.
While this study has focused on the hourly short-term forecasting, producing longer term daily forecast (e.g. 180-270 days ahead) is often required by planners, to support winter planning in ED and Ambulance services which requires more investigation. Moreover, more research is required to forecast other important variables such as length of stay, bed occupancy and waiting time, in addition to patient arrivals and admissions. This may require considering the dynamics among various services including General Practitioners, Emergency Departments, Ambulance and Fire & Rescue services.


# Appendices

## Quantiles of rounded up random variables {#quantilesceiling}

Before proceeding with the proof we need to give the definition of the quantiles of the continuous and rounded up random variables:

\begin{equation} \label{eq:quantCeil1}
	P \left(y_t < k \right) = 1 - \alpha ,
\end{equation}

and

\begin{equation} \label{eq:quantCeil2}
	P \left(\lceil y_t \rceil \leq n \right) \geq 1 - \alpha ,
\end{equation}

where $n$ is the quantile of the distribution of rounded up values (the smallest integer number that satisfies the inequality \eqref{eq:quantCeil2}) and $k$ is the quantile of the continuous distribution of the variable.

In order to prove that $n = \lceil k \rceil$, we need to use the following basic property:

\begin{equation} \label{eq:quantCeil3}
	\lceil y_t \rceil \leq n \iff  y_t \leq n,
\end{equation}

which means that the rounded up value will always be less than or equal to $n$ if and only if the original value is less than or equal to $n$. Taking into account \eqref{eq:quantCeil3}, the probability \eqref{eq:quantCeil2} can be rewritten as:

\begin{equation} \label{eq:quantCeil4}
	P \left(y_t \leq n \right) \geq 1 - \alpha .
\end{equation}

Note also that the following is true:

\begin{equation} \label{eq:quantCeil5}
	P \left(\lceil y_t \rceil \leq n-1 \right) = P \left(y_t \leq n-1 \right) < 1 - \alpha .
\end{equation}

Taking the inequalities \eqref{eq:quantCeil1}, \eqref{eq:quantCeil2}, \eqref{eq:quantCeil4} and \eqref{eq:quantCeil5} into account, the following can be summarised:

\begin{equation} \label{eq:quantCeil6}
	P \left(y_t \leq n-1 \right) < P \left(y_t < k \right) \leq P \left(y_t \leq n \right) ,
\end{equation}

which is possible only when $k \in (n-1, n]$, which means that $\lceil k \rceil = n$. So the rounded up quantile of continuous random variable $y_t$ will always be equal to the quantile of the descritised value of $y_t$:

\begin{equation} \label{eq:ceilingAndQuantiles1}
	\left \lceil Q_\alpha(y_t) \right \rceil = Q_\alpha \left(\lceil y_t \rceil \right) .
\end{equation}

It is also worth noting that the same results can be obtained with the floor function instead of ceiling, following the same logic. So the following equation will hold for all $y_t$ as well:
\begin{equation} \label{eq:floorAndQuantiles1}
	\left \lfloor Q_\alpha(y_t) \right \rfloor = Q_\alpha \left(\lfloor y_t \rfloor \right) .
\end{equation}

# References {#references .unnumbered}


